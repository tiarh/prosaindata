{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNTeij5D8a/dVG9uGA+oSc8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bqGMdzN7rZLs","executionInfo":{"status":"ok","timestamp":1680702602431,"user_tz":-420,"elapsed":43499,"user":{"displayName":"20-180 IKHTIAR HIDAYATULLAH","userId":"10891782236086866315"}},"outputId":"dc81db4b-0de4-437f-afc9-2ad2f6443a51"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"pbOgLvGbrQTP","executionInfo":{"status":"ok","timestamp":1680702694403,"user_tz":-420,"elapsed":636,"user":{"displayName":"20-180 IKHTIAR HIDAYATULLAH","userId":"10891782236086866315"}}},"outputs":[],"source":["import pandas as pd\n","\n","# membaca file Excel\n","df = pd.read_excel('/content/drive/MyDrive/prosaindata/tugas/PTA.xlsx')\n","\n","# melakukan pembersihan data pada kolom Abstrak\n","df['Abstrak'] = df['Abstrak'].str.replace('[^\\w\\s]','', regex=True)\n","\n","# menyimpan hasil ke file Excel baru\n","df.to_excel('Pta_cleaned.xlsx', index=False)\n"]},{"cell_type":"code","source":["import pandas as pd\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","\n","# download stop words dan stemmer\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","# membaca file Excel\n","df = pd.read_excel('/content/Pta_cleaned.xlsx')\n","\n","# menghapus stop words\n","stop_words = set(stopwords.words('indonesian'))\n","df['Abstrak'] = df['Abstrak'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n","\n","# stemming\n","stemmer = PorterStemmer()\n","df['Abstrak'] = df['Abstrak'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n","\n","# tokenization\n","df['Abstrak'] = df['Abstrak'].apply(lambda x: word_tokenize(x))\n","\n","# menyimpan hasil ke file Excel baru\n","df.to_excel('pta_preprocessed.xlsx', index=False)\n","data = pd.read_excel('pta_preprocessed.xlsx')\n","print(data.columns)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"isqb02IGsfTq","executionInfo":{"status":"ok","timestamp":1680705597385,"user_tz":-420,"elapsed":4301,"user":{"displayName":"20-180 IKHTIAR HIDAYATULLAH","userId":"10891782236086866315"}},"outputId":"68403f81-b98a-4945-ff4e-0d0d17916124"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Index(['NPM', 'Judul', 'Abstrak', 'Prodi', 'Label'], dtype='object')\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import nltk\n","from gensim.models import Word2Vec\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","\n","# download punkt tokenizer dari nltk\n","nltk.download('punkt')\n","\n","# membaca file csv\n","df = pd.read_excel('/content/pta_preprocessed.xlsx')\n","\n","# tokenisasi setiap kalimat dalam abstrak\n","sentences = [nltk.word_tokenize(abstrak) for abstrak in df['Abstrak']]\n","\n","# train Word2Vec model\n","model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n","\n","# Transformasi corpus menjadi vektor\n","vectors = model.wv\n","\n","# Lakukan reduksi dimensi pada vektor menggunakan PCA\n","pca = PCA(n_components=50)\n","vectors_pca = pca.fit_transform(vectors.vectors)\n","\n","# Sesuaikan ukuran df['Label'] dengan vectors_pca\n","df = df.iloc[:vectors_pca.shape[0], :]\n","\n","# Pisahkan data menjadi training dan testing\n","X_train, X_test, y_train, y_test = train_test_split(vectors_pca, df['Label'], test_size=0.2)\n","\n","# Klasifikasi dengan naive Bayes\n","clf = MultinomialNB()\n","clf.fit(X_train, y_train)\n","score = clf.score(X_test, y_test)\n","print(\"Akurasi klasifikasi: {:.2f}%\".format(score * 100))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":415},"id":"6ZeeGpuvtCkJ","executionInfo":{"status":"error","timestamp":1680708551832,"user_tz":-420,"elapsed":5924,"user":{"displayName":"20-180 IKHTIAR HIDAYATULLAH","userId":"10891782236086866315"}},"outputId":"4fda8fa3-f8ab-4252-94e1-db578c7cae48"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-47-d42ce8acc1fd>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Pisahkan data menjadi training dan testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors_pca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Klasifikasi dengan naive Bayes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2557\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one array required as input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2559\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [10533, 811]"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.decomposition import PCA\n","\n","# membaca file csv hasil Word2Vec\n","vectors = pd.read_csv('/content/abstract_vectors.csv', index_col=0)\n","\n","# membuat objek PCA dengan 100 komponen\n","pca = PCA(n_components=100)\n","\n","# melakukan fit transform pada vektor\n","vectors_pca = pca.fit_transform(vectors)\n","\n","# menyimpan hasil reduksi dimensi ke dalam file csv\n","vectors_pca = pd.DataFrame(vectors_pca, index=vectors.index)\n","vectors_pca.to_csv('pta_vectors_pca.csv')\n","# load dataset\n","data = pd.read_csv('pta_vectors_pca.csv')\n","\n","# cek kolom pada dataset\n","print(data.columns)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gaZQdtiUyIkq","executionInfo":{"status":"ok","timestamp":1680704739035,"user_tz":-420,"elapsed":3276,"user":{"displayName":"20-180 IKHTIAR HIDAYATULLAH","userId":"10891782236086866315"}},"outputId":"db1a415d-c7de-4a1d-dc3e-289236bb3ebd"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['Unnamed: 0', '0', '1', '2', '3', '4', '5', '6', '7', '8',\n","       ...\n","       '90', '91', '92', '93', '94', '95', '96', '97', '98', '99'],\n","      dtype='object', length=101)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score\n","\n","# membaca file csv\n","df = pd.read_csv('/content/pta_vectors_pca.csv', index_col=0)\n","\n","# memisahkan kolom label sebagai target\n","target = df.index.values\n","data = df.values\n","\n","# membagi data menjadi train dan test\n","X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n","\n","# inisialisasi model Naive Bayes\n","nb = GaussianNB()\n","\n","# training model dengan data train\n","nb.fit(X_train, y_train)\n","\n","# membuat prediksi dengan data test\n","y_pred = nb.predict(X_test)\n","\n","# menghitung akurasi\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Akurasi:\", accuracy)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0I4ZkhhEy7xT","executionInfo":{"status":"ok","timestamp":1680705454196,"user_tz":-420,"elapsed":15467,"user":{"displayName":"20-180 IKHTIAR HIDAYATULLAH","userId":"10891782236086866315"}},"outputId":"6d30e313-eb0e-497b-d853-3f4ffd6e3e01"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Akurasi: 0.0\n"]}]}]}